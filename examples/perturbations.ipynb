{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l98gJVtFFJPB"
      },
      "source": [
        "We review in this notebook a universal method to transform any function $f$ mapping a pytree to another pytree to a differentiable approximation $f_\\varepsilon$, using pertutbations following the method of [Berthet et al. (2020)](https://arxiv.org/abs/2002.08676).\n",
        "\n",
        "For a random $Z$ drawn from a distribution with continuous positive distribution $\\mu$ and a function $f: X \\to Y$, its perturbed approximation defined for any $x \\in X$ by\n",
        "\n",
        "$$f_\\varepsilon(x) = \\mathbf{E}[f (x + \\varepsilon Z )]\\, .$$\n",
        "\n",
        "We illustrate here on some examples, including the case of an optimizer function $y^*$ over $C$ defined for any cost $\\theta \\in \\mathbb{R}^d$ by\n",
        "\n",
        "$$y^*(\\theta) = \\mathop{\\mathrm{arg\\,max}}_{y \\in C} \\langle y, \\theta \\rangle\\, .$$\n",
        "\n",
        "In this case, the perturbed optimizer is given by\n",
        "\n",
        "$$y_\\varepsilon^*(\\theta) = \\mathbf{E}[\\mathop{\\mathrm{arg\\,max}}_{y\\in C} \\langle y, \\theta + \\varepsilon Z \\rangle]\\, .$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WIWwRdSU51j"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install jaxopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIUNtyaCFJPB"
      },
      "outputs": [],
      "source": [
        "# activate TPUs if available\n",
        "try:\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "except (KeyError, RuntimeError):\n",
        "    print(\"TPU not found, continuing without it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6tLyyy9VCEw"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import operator\n",
        "from jax import tree_util as jtu\n",
        "\n",
        "from optax import tree_utils as otu\n",
        "from optax import perturbations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmYn_jNUFfw2"
      },
      "source": [
        "# Argmax one-hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmHzgCn7FJPC"
      },
      "source": [
        "We consider an optimizer, such as the following `argmax_one_hot` function. It transforms a real-valued vector into a binary vector with a 1 in the coefficient with largest magnitude and 0 elsewhere. It corresponds to $y^*$ for $C$ being the unit simplex. We run it on an example input `values`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84N-wAJ8GDK2"
      },
      "source": [
        "## One-hot function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMZnzhX4FjGj"
      },
      "outputs": [],
      "source": [
        "def argmax_one_hot(x, axis=-1):\n",
        "  return jax.nn.one_hot(jnp.argmax(x, axis=axis), x.shape[axis])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iynCk8734Wiz"
      },
      "outputs": [],
      "source": [
        "values = jnp.array([-0.6, 1.9, -0.2, 1.1, -1.0])\n",
        "\n",
        "one_hot_vec = argmax_one_hot(values)\n",
        "print(one_hot_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rbNt-6zGb-J"
      },
      "source": [
        "## One-hot with pertubations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lvIhCV1FJPD"
      },
      "source": [
        "Our implementation transforms the `argmax_one_hot` function into a perturbed one that we call `pert_one_hot`. In this case we use Gumbel noise for the perturbation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hQz6zuPwkpZ"
      },
      "outputs": [],
      "source": [
        "N_SAMPLES = 100\n",
        "SIGMA = 0.5\n",
        "GUMBEL = perturbations.Gumbel()\n",
        "\n",
        "rng = jax.random.PRNGKey(1)\n",
        "pert_one_hot = perturbations.make_perturbed_fun(fun=argmax_one_hot,\n",
        "                                                num_samples=N_SAMPLES,\n",
        "                                                sigma=SIGMA,\n",
        "                                                noise=GUMBEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2VBjnSUFJPD"
      },
      "source": [
        "In this particular case, it is equal to the usual [softmax function](https://en.wikipedia.org/wiki/Softmax_function). This is not always true, in general there is no closed form for $y_\\varepsilon^*$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2gDpghJYZ33"
      },
      "outputs": [],
      "source": [
        "rngs = jax.random.split(rng, 2)\n",
        "\n",
        "rng = rngs[0]\n",
        "\n",
        "pert_argmax = pert_one_hot(values, rng)\n",
        "print(f'computation with {N_SAMPLES} samples, sigma = {SIGMA}')\n",
        "print(f'perturbed argmax = {pert_argmax}')\n",
        "jax.nn.softmax(values/SIGMA)\n",
        "soft_max = jax.nn.softmax(values/SIGMA)\n",
        "print(f'softmax = {soft_max}')\n",
        "print(f'square norm of softmax = {jnp.linalg.norm(soft_max):.2e}')\n",
        "print(f'square norm of difference = {jnp.linalg.norm(pert_argmax - soft_max):.2e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U7rhtEAGpMV"
      },
      "source": [
        "## Gradients for one-hot with perturbations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldxsWvLmFJPD"
      },
      "source": [
        "The perturbed optimizer $y_\\varepsilon^*$ is differentiable, and its gradient can be computed with stochastic estimation automatically, using `jax.grad`.\n",
        "\n",
        "We create a scalar loss `loss_simplex` of the perturbed optimizer $y^*_\\varepsilon$\n",
        "\n",
        "$$\\ell_\\text{simplex}(y_{\\text{true}} = y_\\varepsilon^*; y_{\\text{true}})$$  \n",
        "\n",
        "For `values` equal to a vector $\\theta$, we can compute gradients of\n",
        "\n",
        "$$\\ell(\\theta) = \\ell_\\text{simplex}(y_\\varepsilon^*(\\theta); y_{\\text{true}})$$\n",
        "with respect to `values`, automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H1LD4QhGtFI"
      },
      "outputs": [],
      "source": [
        "# Example loss function\n",
        "\n",
        "def loss_simplex(values, rng):\n",
        "  n = values.shape[0]\n",
        "  v_true = jnp.arange(n) + 2\n",
        "  y_true = v_true / jnp.sum(v_true)\n",
        "  y_pred = pert_one_hot(values, rng)\n",
        "  return jnp.sum((y_true - y_pred) ** 2)\n",
        "\n",
        "loss_simplex(values, rngs[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM2poXb4FJPD"
      },
      "source": [
        "We can compute the gradient of $\\ell$ directly\n",
        "\n",
        "$$\\nabla_\\theta \\ell(\\theta) = \\partial_\\theta y^*_\\varepsilon(\\theta) \\cdot \\nabla_1 \\ell_{\\text{simplex}}(y^*_\\varepsilon(\\theta); y_{\\text{true}})$$\n",
        "\n",
        "The computation of the jacobian $\\partial_\\theta y^*_\\varepsilon(\\theta)$ is implemented automatically, using an estimation method given by [Berthet et al. (2020)](https://arxiv.org/abs/2002.08676), [Prop. 3.1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjQatCE3GtFJ"
      },
      "outputs": [],
      "source": [
        "# Gradient of the loss w.r.t input values\n",
        "\n",
        "gradient = jax.grad(loss_simplex)(values, rngs[1])\n",
        "print(gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh2Qt97AFJPD"
      },
      "source": [
        "We illustrate the use of this method by running 200 steps of gradient descent on $\\theta_t$ so that it minimizes this loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuNE2RX0GtFJ"
      },
      "outputs": [],
      "source": [
        "# Doing 200 steps of gradient descent on the values to have the desired ranks\n",
        "\n",
        "steps = 200\n",
        "values_t = values\n",
        "eta = 0.5\n",
        "\n",
        "grad_func = jax.jit(jax.grad(loss_simplex))\n",
        "\n",
        "for t in range(steps):\n",
        "  rngs = jax.random.split(rngs[1], 2)\n",
        "  values_t = values_t - eta * grad_func(values_t, rngs[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29TWHiH0GtFJ"
      },
      "outputs": [],
      "source": [
        "rngs = jax.random.split(rngs[1], 2)\n",
        "\n",
        "n = values.shape[0]\n",
        "v_true = jnp.arange(n) + 2\n",
        "y_true = v_true / jnp.sum(v_true)\n",
        "\n",
        "print(f'initial values = {values}')\n",
        "print(f'initial one-hot = {argmax_one_hot(values)}')\n",
        "print(f'initial diff. one-hot = {pert_one_hot(values, rngs[0])}')\n",
        "print()\n",
        "print(f'values after GD = {values_t}')\n",
        "print(f'ranks after GD = {argmax_one_hot(values_t)}')\n",
        "print(f'diff. one-hot after GD = {pert_one_hot(values_t, rngs[1])}')\n",
        "print(f'target diff. one-hot = {y_true}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Vyh_a1bZT-s"
      },
      "source": [
        "# Differentiable ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmVAjbJxFzUA"
      },
      "source": [
        "## Ranking function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyapGu77FJPE"
      },
      "source": [
        "We consider an optimizer, such as the following `ranking` function. It transforms a real-valued vector of size $n$ into a vector with coefficients being a permutation of $\\{0,\\ldots, n-1\\}$ corresponding to the order of the coefficients of the original vector. It corresponds to $y^*$ for $C$ being the permutahedron. We run it on an example input `values`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NKbR6TlZUTG"
      },
      "outputs": [],
      "source": [
        "# Function outputting a vector of ranks\n",
        "\n",
        "def ranking(values):\n",
        "  return jnp.argsort(jnp.argsort(values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU69uMAoZncY"
      },
      "outputs": [],
      "source": [
        "# Example on random values\n",
        "\n",
        "n = 6\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "values = jax.random.normal(rng, (n,))\n",
        "\n",
        "print(f'values = {values}')\n",
        "print(f'ranking = {ranking(values)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j1Vgfz_bb9u"
      },
      "source": [
        "## Ranking with perturbations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu2wfbNuFJPE"
      },
      "source": [
        "As above, our implementation transforms this function into a perturbed one that we call `pert_ranking`. In this case we use Gumbel noise for the perturbation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Equ3_gDPbf5n"
      },
      "outputs": [],
      "source": [
        "N_SAMPLES = 100\n",
        "SIGMA = 0.2\n",
        "GUMBEL = perturbations.Gumbel()\n",
        "\n",
        "pert_ranking = perturbations.make_perturbed_fun(ranking,\n",
        "                                                num_samples=N_SAMPLES,\n",
        "                                                sigma=SIGMA,\n",
        "                                                noise=GUMBEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMj-Dnudby_a"
      },
      "outputs": [],
      "source": [
        "# Expectation of the perturbed ranks on these values\n",
        "\n",
        "rngs = jax.random.split(rng, 2)\n",
        "\n",
        "diff_ranks = pert_ranking(values, rngs[0])\n",
        "print(f'values = {values}')\n",
        "\n",
        "print(f'diff_ranks = {diff_ranks}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH6Ew85koQvU"
      },
      "source": [
        "## Gradients for ranking with perturbations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDZOEt18FJPE"
      },
      "source": [
        "As above, the perturbed optimizer $y_\\varepsilon^*$ is differentiable, and its gradient can be computed with stochastic estimation automatically, using `jax.grad`.\n",
        "\n",
        "We showcase this on a loss of $y_\\varepsilon(\\theta)$ that can be directly differentiated w.r.t. the `values` equal to $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-T8y6N8cHzF"
      },
      "outputs": [],
      "source": [
        "# Example loss function\n",
        "\n",
        "def loss_example(values, rng):\n",
        "  n = values.shape[0]\n",
        "  y_true = ranking(jnp.arange(n))\n",
        "  y_pred = pert_ranking(values, rng)\n",
        "  return jnp.sum((y_true - y_pred) ** 2)\n",
        "\n",
        "print(loss_example(values, rngs[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7nzNwP-e68q"
      },
      "outputs": [],
      "source": [
        "# Gradient of the objective w.r.t input values\n",
        "\n",
        "gradient = jax.grad(loss_example)(values, rngs[1])\n",
        "print(gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC7IzKADFJPE"
      },
      "source": [
        "As above, we showcase this example on gradient descent to minimize this loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UObBP3QfCqq"
      },
      "outputs": [],
      "source": [
        "steps = 20\n",
        "values_t = values\n",
        "eta = 0.1\n",
        "\n",
        "grad_func = jax.jit(jax.grad(loss_example))\n",
        "\n",
        "for t in range(steps):\n",
        "  rngs = jax.random.split(rngs[1], 2)\n",
        "  values_t = values_t - eta * grad_func(values_t, rngs[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4iNxMoQmZRa"
      },
      "outputs": [],
      "source": [
        "rngs = jax.random.split(rngs[1], 2)\n",
        "\n",
        "y_true = ranking(jnp.arange(n))\n",
        "\n",
        "print(f'initial values = {values}')\n",
        "print(f'initial ranks = {ranking(values)}')\n",
        "print(f'initial diff. ranks = {pert_ranking(values, rngs[0])}')\n",
        "print()\n",
        "print(f'values after GD = {values_t}')\n",
        "print(f'ranks after GD = {ranking(values_t)}')\n",
        "print(f'diff. ranks after GD = {pert_ranking(values_t, rngs[1])}')\n",
        "print(f'target diff. ranks = {y_true}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P537S89ZlDQR"
      },
      "source": [
        "# General input / outputs (Pytrees)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V62NPuUvSHk8"
      },
      "source": [
        "This method can be applied to any function taking pytrees as input and output in the forward mode, and can also be used to compute derivatives, as illustrated below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bz35ZWQpeB7"
      },
      "outputs": [],
      "source": [
        "tree_a = (jnp.array((0.1, 0.4, 0.5)),\n",
        "          {'k1': jnp.array((0.1, 0.2)),\n",
        "           'k2': jnp.array((0.1, 0.1))},\n",
        "          jnp.array((0.4, 0.3, 0.2, 0.1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxcczrOZhCZJ"
      },
      "source": [
        "## Tree argmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUQqWJYfgrag"
      },
      "source": [
        "This piecewise constant function applies the argmax to every leaf array of the pytree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szID1S5Jg_LL"
      },
      "outputs": [],
      "source": [
        "argmax_tree = lambda x: jtu.tree_map(argmax_one_hot, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPfzjdSJxP4G"
      },
      "outputs": [],
      "source": [
        "argmax_tree(tree_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOmGdkW0g2-6"
      },
      "source": [
        "The perturbed approximation applies a perturbed softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKuD_cElxSDd"
      },
      "outputs": [],
      "source": [
        "N_SAMPLES = 100\n",
        "sigma = 1.0\n",
        "\n",
        "pert_argmax_fun = perturbations.make_perturbed_fun(argmax_tree,\n",
        "                                                   num_samples=N_SAMPLES,\n",
        "                                                   sigma=SIGMA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fnpfpVBxYSQ"
      },
      "outputs": [],
      "source": [
        "pert_argmax_fun(tree_a, rng)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQOh_iZmhLVS"
      },
      "source": [
        "## Scalar loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW0U0DW1xbAV"
      },
      "outputs": [],
      "source": [
        "def pert_loss(inputs, rng):\n",
        "  pert_softmax = pert_argmax_fun(inputs, rng)\n",
        "  argmax = argmax_tree(inputs)\n",
        "  diffs = jtu.tree_map(lambda x, y: jnp.sum((x - y) ** 2 / 4), argmax, pert_softmax)\n",
        "  return jtu.tree_reduce(operator.add, diffs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWjXKeMSxodX"
      },
      "outputs": [],
      "source": [
        "init_loss = pert_loss(tree_a, rng)\n",
        "\n",
        "print(f'initial loss value = {init_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXobsx6bhRb8"
      },
      "source": [
        "## Gradient computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kydUMAachVgp"
      },
      "source": [
        "The gradient of the scalar loss can be evaluated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vryBVzPsxqlI"
      },
      "outputs": [],
      "source": [
        "grad = jax.grad(pert_loss)(tree_a, rng)\n",
        "\n",
        "print('Gradient of the scalar loss')\n",
        "print()\n",
        "grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpbnFapshcaM"
      },
      "source": [
        "A small step in the gradient direction reduces the value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkwIh76L1Azl"
      },
      "outputs": [],
      "source": [
        "eta = 1e-1\n",
        "\n",
        "loss_step = pert_loss(otu.tree_add_scalar_mul(tree_a, -eta, grad), rng)\n",
        "\n",
        "print(f'initial loss value = {init_loss:.3f}')\n",
        "print(f'loss after gradient step = {loss_step:.3f}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1CDnfXElxMMd_144LVTaeVCLay3-RXJrA",
          "timestamp": 1726135261814
        },
        {
          "file_id": "1i83GFtgxkGQ6t-WTG0Bz9SMGMvzmVfc7",
          "timestamp": 1726129388187
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
